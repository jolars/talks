@article{beck2009,
  title         = {A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems},
  author        = {Beck, A. and Teboulle, M.},
  volume        = {2},
  number        = {1},
  pages         = {183--202},
  doi           = {10.1137/080716542},
  url           = {https://epubs.siam.org/doi/abs/10.1137/080716542},
  urldate       = {2019-02-10},
  date          = {2009-01-01},
  journaltitle  = {SIAM Journal on Imaging Sciences},
  shortjournal  = {SIAM J. Imaging Sci.},
  abstract      = {
    We consider the class of iterative shrinkage-thresholding algorithms (ISTA) for
    solving linear inverse problems arising in signal/image processing. This class of
    methods, which can be viewed as an extension of the classical gradient algorithm,
    is attractive due to its simplicity and thus is adequate for solving large-scale
    problems even with dense matrix data. However, such methods are also known to
    converge quite slowly. In this paper we present a new fast iterative
    shrinkage-thresholding algorithm (FISTA) which preserves the computational
    simplicity of ISTA but with a global rate of convergence which is proven to be
    significantly better, both theoretically and practically. Initial promising
    numerical results for wavelet-based image deblurring demonstrate the capabilities
    of FISTA which is shown to be faster than ISTA by several orders of magnitude.
  }
}

% == BibLateX quality report for beck2009:
% ? unused Library catalog ("epubs.siam.org (Atypon)")
@article{beck2013a,
  title         = {On the Convergence of Block Coordinate Descent Type Methods},
  author        = {Beck, Amir and Tetruashvili, Luba},
  publisher     = {{Society for Industrial and Applied Mathematics}},
  volume        = {23},
  number        = {4},
  pages         = {2037--2060},
  doi           = {10.1137/120887679},
  issn          = {1052-6234},
  url           = {https://epubs-siam-org.ludwig.lub.lu.se/doi/10.1137/120887679},
  urldate       = {2022-02-03},
  date          = {2013-01-01},
  journaltitle  = {SIAM Journal on Optimization},
  shortjournal  = {SIAM J. Optim.},
  abstract      = {
    In this paper we study smooth convex programming problems where the decision
    variables vector is split into several blocks of variables. We analyze the block
    coordinate gradient projection method in which each iteration consists of
    performing a gradient projection step with respect to a certain block taken in a
    cyclic order. Global sublinear rate of convergence of this method is established
    and it is shown that it can be accelerated when the problem is unconstrained. In
    the unconstrained setting we also prove a sublinear rate of convergence result for
    the so-called alternating minimization method when the number of blocks is two.
    When the objective function is also assumed to be strongly convex, linear rate of
    convergence is established.
  }
}

% == BibLateX quality report for beck2013a:
% Unexpected field 'publisher'
% ? unused Library catalog ("epubs-siam-org.ludwig.lub.lu.se (Atypon)")
@unpublished{bogdan2013,
  title         = {Statistical Estimation and Testing via the Sorted {{L1}} Norm},
  author        = {
    Bogdan, Ma\l{}gorzata and family=Berg, given=Ewout, prefix=van den, useprefix=false
    and Su, Weijie and Cand\`{e}s, Emmanuel
  },
  url           = {http://arxiv.org/abs/1310.1969},
  urldate       = {2020-04-16},
  date          = {2013-10-29},
  eprint        = {1310.1969},
  eprinttype    = {arxiv},
  eprintclass   = {math, stat},
  abstract      = {
    We introduce a novel method for sparse regression and variable selection, which is
    inspired by modern ideas in multiple testing. Imagine we have observations from the
    linear model y = X beta + z, then we suggest estimating the regression coefficients
    by means of a new estimator called SLOPE, which is the solution to minimize 0.5
    \vert{}\vert{}y - Xb\textbackslash \vert{}\_2\^2 + lambda\_1 \vert{}b\vert{}\_(1) +
    lambda\_2 \vert{}b\vert{} \_(2) + ... + lambda\_p \vert{}b\vert{}\_(p); here,
    lambda\_1 {$>$}= \textbackslash lambda\_2 {\$>\$}= ... {\$>\$}= \textbackslash
    lambda\_p {\$>\$}= 0 and \vert{}b\vert{}\_(1) {\$>\$}= \vert{}b\vert{}\_(2)
    {\$>\$}= ... {\$>\$}= \vert{}b\vert{}\_(p) is the order statistic of the magnitudes
    of b. The regularizer is a sorted L1 norm which penalizes the regression
    coefficients according to their rank: the higher the rank, the larger the penalty.
    This is similar to the famous BHq procedure [Benjamini and Hochberg, 1995], which
    compares the value of a test statistic taken from a family to a critical threshold
    that depends on its rank in the family. SLOPE is a convex program and we
    demonstrate an efficient algorithm for computing the solution. We prove that for
    orthogonal designs with p variables, taking lambda\_i = F\^\{-1\} (1-q\_i) (F is
    the cdf of the errors), q\_i = iq/(2p), controls the false discovery rate (FDR) for
    variable selection. When the design matrix is nonorthogonal there are inherent
    limitations on the FDR level and the power which can be obtained with model
    selection methods based on L1-like penalties. However, whenever the columns of the
    design matrix are not strongly correlated, we demonstrate empirically that it is
    possible to select the parameters lambda\_i as to obtain FDR control at a
    reasonable level as long as the number of nonzero coefficients is not too large. At
    the same time, the procedure exhibits increased power over the lasso, which treats
    all coefficients equally. The paper illustrates further estimation properties of
    the new selection rule through comprehensive simulation studies.
  }
}

@article{bogdan2015,
  title         = {{{SLOPE}} ‚Äì Adaptive Variable Selection via Convex Optimization},
  author        = {
    Bogdan, Ma\l{}gorzata and family=Berg, given=Ewout, prefix=van den, useprefix=false
    and Sabatti, Chiara and Su, Weijie and Cand\`{e}s, Emmanuel J.
  },
  volume        = {9},
  number        = {3},
  pages         = {1103--1140},
  doi           = {10.1214/15-AOAS842},
  issn          = {1932-6157},
  url           = {https://projecteuclid.org/euclid.aoas/1446488733},
  urldate       = {2018-12-17},
  date          = {2015-09},
  journaltitle  = {The annals of applied statistics},
  shortjournal  = {Ann Appl Stat},
  eprint        = {26709357},
  eprinttype    = {pmid}
}

% == BibLateX quality report for bogdan2015:
% ? unused Library catalog ("PubMed Central")
@unpublished{bogdan2022,
  title         = {Pattern Recovery by {{SLOPE}}},
  author        = {
    Bogdan, Ma\l{}gorzata and Dupuis, Xavier and Graczyk, Piotr and Ko\l{}odziejek,
    Bartosz and Skalski, Tomasz and Tardivel, Patrick and Wilczy\'{n}ski, Maciej
  },
  publisher     = {{arXiv}},
  number        = {arXiv:2203.12086},
  pages         = {27},
  doi           = {10.48550/arXiv.2203.12086},
  url           = {http://arxiv.org/abs/2203.12086},
  urldate       = {2022-06-03},
  date          = {2022-05-17},
  eprint        = {2203.12086},
  eprinttype    = {arxiv},
  eprintclass   = {math, stat},
  abstract      = {
    LASSO and SLOPE are two popular methods for dimensionality reduction in the
    high-dimensional regression. LASSO can eliminate redundant predictors by setting
    the corresponding regression coefficients to zero, while SLOPE can additionally
    identify clusters of variables with the same absolute values of regression
    coefficients. It is well known that LASSO Irrepresentability Condition is
    sufficient and necessary for the proper estimation of the sign of sufficiently
    large regression coefficients. In this article we formulate an analogous
    Irrepresentability Condition for SLOPE, which is sufficient and necessary for the
    proper identification of the SLOPE pattern, i.e. of the proper sign as well as of
    the proper ranking of the absolute values of individual regression coefficients,
    while proper ranking guarantees a proper clustering. We also provide asymptotic
    results on the strong consistency of pattern recovery by SLOPE when the number of
    columns in the design matrix is fixed while the sample size diverges to infinity.
  }
}

% == BibLateX quality report for bogdan2022:
% Unexpected field 'number'
% Unexpected field 'pages'
% Unexpected field 'publisher'
@article{boyd2010,
  title         = {
    Distributed Optimization and Statistical Learning via the Alternating Direction
    Method of Multipliers
  },
  author        = {
    Boyd, Stephen and Parikh, Neil and Chu, Eric and Peleato, Borja and Eckstein,
    Jonathan
  },
  volume        = {3},
  number        = {1},
  pages         = {1--122},
  doi           = {10.1561/2200000016},
  issn          = {1935-8237, 1935-8245},
  url           = {http://www.nowpublishers.com/article/Details/MAL-016},
  urldate       = {2020-02-07},
  date          = {2010},
  journaltitle  = {Foundations and Trends\textregistered{} in Machine Learning},
  shortjournal  = {FNT in Machine Learning},
  langid        = {english}
}

% == BibLateX quality report for boyd2010:
% 'issn': not a valid ISSN
% ? unused Library catalog ("DOI.org (Crossref)")
@online{boyd2011,
  title         = {{{MATLAB}} Scripts for Alternating Direction Method of Multipliers},
  author        = {Boyd, Stephen and Parikh, N. and Chu, E. and Peleato, B. and Eckstein, J.},
  url           = {https://web.stanford.edu/~boyd/papers/admm/},
  urldate       = {2022-10-11},
  date          = {2011-04-26},
  organization  = {{Stanford University}}
}

@article{breheny2011,
  title         = {
    Coordinate Descent Algorithms for Nonconvex Penalized Regression, with Applications
    to Biological Feature Selection
  },
  author        = {Breheny, Patrick and Huang, Jian},
  volume        = {5},
  number        = {1},
  pages         = {232--253},
  doi           = {10.1214/10-AOAS388},
  issn          = {1932-6157, 1941-7330},
  url           = {https://projecteuclid.org/euclid.aoas/1300715189},
  urldate       = {2018-03-12},
  date          = {2011-03},
  journaltitle  = {The Annals of Applied Statistics},
  shortjournal  = {Ann. Appl. Stat.},
  abstract      = {
    A number of variable selection methods have been proposed involving nonconvex
    penalty functions. These methods, which include the smoothly clipped absolute
    deviation (SCAD) penalty and the minimax concave penalty (MCP), have been
    demonstrated to have attractive theoretical properties, but model fitting is not a
    straightforward task, and the resulting solutions may be unstable. Here, we
    demonstrate the potential of coordinate descent algorithms for fitting these
    models, establishing theoretical convergence properties and demonstrating that they
    are significantly faster than competing approaches. In addition, we demonstrate the
    utility of convexity diagnostics to determine regions of the parameter space in
    which the objective function is locally convex, even though the penalty is not. Our
    simulation study and data examples indicate that nonconvex penalties like MCP and
    SCAD are worthwhile alternatives to the lasso in many applications. In particular,
    our numerical results suggest that MCP is the preferred approach among the three
    methods.
  },
  langid        = {english},
  mrnumber      = {MR2810396},
  zmnumber      = {1220.62095}
}

% == BibLateX quality report for breheny2011:
% Unexpected field 'mrnumber'
% Unexpected field 'zmnumber'
% 'issn': not a valid ISSN
% ? unused Library catalog ("Project Euclid")
@article{daubechies2004,
  title         = {
    An Iterative Thresholding Algorithm for Linear Inverse Problems with a Sparsity
    Constraint
  },
  author        = {Daubechies, I. and Defrise, M. and De Mol, C.},
  volume        = {57},
  number        = {11},
  pages         = {1413--1457},
  doi           = {10.1002/cpa.20042},
  issn          = {1097-0312},
  url           = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.20042},
  urldate       = {2022-05-27},
  date          = {2004-08-26},
  journaltitle  = {Communications on Pure and Applied Mathematics},
  abstract      = {
    We consider linear inverse problems where the solution is assumed to have a sparse
    expansion on an arbitrary preassigned orthonormal basis. We prove that replacing
    the usual quadratic regularizing penalties by weighted ùìÅp-penalties on the
    coefficients of such expansions, with 1 \leq{} p \leq{} 2, still regularizes the
    problem. Use of such ùìÅp-penalized problems with p {$<$} 2 is often advocated when
    one expects the underlying ideal noiseless solution to have a sparse expansion with
    respect to the basis under consideration. To compute the corresponding regularized
    solutions, we analyze an iterative algorithm that amounts to a Landweber iteration
    with thresholding (or nonlinear shrinkage) applied at each iteration step. We prove
    that this algorithm converges in norm. \textcopyright{} 2004 Wiley Periodicals,
    Inc.
  },
  langid        = {english}
}

% == BibLateX quality report for daubechies2004:
% ? unused extra: _eprint ("https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpa.20042")
% ? unused Library catalog ("Wiley Online Library")
@unpublished{elvira2022,
  title         = {
    Safe Rules for the Identification of Zeros in the Solutions of the {{ SLOPE}}
    Problem
  },
  author        = {Elvira, Cl\'{e}ment and Herzet, C\'{e}dric},
  publisher     = {{arXiv}},
  number        = {arXiv:2110.11784},
  doi           = {10.48550/arXiv.2110.11784},
  url           = {http://arxiv.org/abs/2110.11784},
  urldate       = {2022-06-03},
  date          = {2022-04-18},
  eprint        = {2110.11784},
  eprinttype    = {arxiv},
  eprintclass   = {cs},
  abstract      = {
    In this paper we propose a methodology to accelerate the resolution of the
    so-called "Sorted L-One Penalized Estimation" (SLOPE) problem. Our method leverages
    the concept of "safe screening", well-studied in the literature for \textbackslash
    textit\{group-separable\} sparsity-inducing norms, and aims at identifying the
    zeros in the solution of SLOPE. More specifically, we derive a set of
    \textbackslash (\textbackslash tfrac\{n(n+1)\}\{ 2\}\textbackslash ) inequalities
    for each element of the \textbackslash (n\textbackslash )-dimensional primal vector
    and prove that the latter can be safely screened if some subsets of these
    inequalities are verified. We propose moreover an efficient algorithm to jointly
    apply the proposed procedure to all the primal variables. Our procedure has a
    complexity \textbackslash ( \textbackslash mathcal\{O\}(n\textbackslash log n + LT)
    \textbackslash ) where \textbackslash (T\textbackslash leq n \textbackslash ) is a
    problem-dependent constant and \textbackslash (L\textbackslash ) is the number of
    zeros identified by the tests. Numerical experiments confirm that, for a prescribed
    computational budget, the proposed methodology leads to significant improvements of
    the solving precision.
  }
}

% == BibLateX quality report for elvira2022:
% Unexpected field 'number'
% Unexpected field 'publisher'
@article{fan2001,
  title         = {Variable Selection via Nonconcave Penalized Likelihood and Its Oracle Properties},
  author        = {Fan, Jianqing and Li, Runze},
  volume        = {96},
  number        = {456},
  pages         = {1348--1360},
  doi           = {10/fd7bfs},
  issn          = {0162-1459},
  url           = {https://doi.org/10.1198/016214501753382273},
  urldate       = {2018-03-14},
  date          = {2001-12-01},
  journaltitle  = {Journal of the American Statistical Association},
  abstract      = {
    Variable selection is fundamental to high-dimensional statistical modeling,
    including nonparametric regression. Many approaches in use are stepwise selection
    procedures, which can be computationally expensive and ignore stochastic errors in
    the variable selection process. In this article, penalized likelihood approaches
    are proposed to handle these kinds of problems. The proposed methods select
    variables and estimate coefficients simultaneously. Hence they enable us to
    construct confidence intervals for estimated parameters. The proposed approaches
    are distinguished from others in that the penalty functions are symmetric,
    nonconcave on (0, \infty{}), and have singularities at the origin to produce sparse
    solutions. Furthermore, the penalty functions should be bounded by a constant to
    reduce bias and satisfy certain conditions to yield continuous solutions. A new
    algorithm is proposed for optimizing penalized likelihood functions. The proposed
    ideas are widely applicable. They are readily applied to a variety of parametric
    models such as generalized linear models and robust regression models. They can
    also be applied easily to nonparametric modeling by using wavelets and splines.
    Rates of convergence of the proposed penalized likelihood estimators are
    established. Furthermore, with proper choice of regularization parameters, we show
    that the proposed estimators perform as well as the oracle procedure in variable
    selection; namely, they work as well as if the correct submodel were known. Our
    simulation shows that the newly proposed methods compare favorably with other
    variable selection techniques. Furthermore, the standard error formulas are tested
    to be accurate enough for practical applications.
  }
}

% == BibLateX quality report for fan2001:
% ? unused Library catalog ("Taylor and Francis+NEJM")
@unpublished{figueiredo2014,
  title         = {
    Sparse Estimation with Strongly Correlated Variables Using Ordered Weighted {{L1}}
    Regularization
  },
  author        = {Figueiredo, Mario A. T. and Nowak, Robert D.},
  publisher     = {{arXiv}},
  number        = {arXiv:1409.4005},
  doi           = {10.48550/arXiv.1409.4005},
  url           = {http://arxiv.org/abs/1409.4005},
  urldate       = {2022-06-03},
  date          = {2014-09-13},
  eprint        = {1409.4005},
  eprinttype    = {arxiv},
  eprintclass   = {stat},
  abstract      = {
    This paper studies ordered weighted L1 (OWL) norm regularization for sparse
    estimation problems with strongly correlated variables. We prove sufficient
    conditions for clustering based on the correlation/colinearity of variables using
    the OWL norm, of which the so-called OSCAR is a particular case. Our results extend
    previous ones for OSCAR in several ways: for the squared error loss , our
    conditions hold for the more general OWL norm and under weaker assumptions; we also
    establish clustering conditions for the absolute error loss, which is, as far as we
    know, a novel result. Furthermore, we characterize the statistical performance of
    OWL norm regularization for generative models in which certain clusters of
    regression variables are strongly (even perfectly) correlated, but variables in
    different clusters are uncorrelated. We show that if the true p-dimensional signal
    generating the data involves only s of the clusters, then O(s log p) samples
    suffice to accurately estimate the signal, regardless of the number of coefficients
    within the clusters. The estimation of s-sparse signals with completely independent
    variables requires just as many measurements. In other words, using the OWL we pay
    no price (in terms of the number of measurements) for the presence of strongly
    correlated variables.
  }
}

% == BibLateX quality report for figueiredo2014:
% Unexpected field 'number'
% Unexpected field 'publisher'
@inproceedings{figueiredo2016,
  title         = {
    Ordered Weighted {{L1}} Regularized Regression with Strongly Correlated Covariates:
    Theoretical Aspects
  },
  shorttitle    = {
    Ordered {{Weighted L1 Regularized Regression}} with {{Strongly Correlated
    Covariates}}
  },
  author        = {Figueiredo, Mario and Nowak, Robert},
  booktitle     = {Artificial {{Intelligence}} and {{Statistics}}},
  pages         = {930--938},
  url           = {http://proceedings.mlr.press/v51/figueiredo16.html},
  urldate       = {2019-11-05},
  date          = {2016-05-02},
  abstract      = {
    This paper studies the ordered weighted L1 (OWL) family of regularizers for sparse
    linear regression with strongly correlated covariates. We prove sufficient
    conditions for clustering correlated c...
  },
  eventtitle    = {Artificial {{Intelligence}} and {{Statistics}}},
  langid        = {english}
}

% == BibLateX quality report for figueiredo2016:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("proceedings.mlr.press")
@article{friedman2007,
  title         = {Pathwise Coordinate Optimization},
  author        = {Friedman, Jerome and Hastie, Trevor and H\"{o}fling, Holger and Tibshirani, Robert},
  volume        = {1},
  number        = {2},
  pages         = {302--332},
  doi           = {10/d88g8c},
  issn          = {1932-6157},
  url           = {https://projecteuclid.org/euclid.aoas/1196438020},
  urldate       = {2018-03-12},
  date          = {2007-12},
  journaltitle  = {The Annals of Applied Statistics},
  shortjournal  = {Ann. Appl. Stat.},
  abstract      = {
    We consider ``one-at-a-time'' coordinate-wise descent algorithms for a class of
    convex optimization problems. An algorithm of this kind has been proposed for the
    L1-penalized regression (lasso) in the literature, but it seems to have been
    largely ignored. Indeed, it seems that coordinate-wise algorithms are not often
    used in convex optimization. We show that this algorithm is very competitive with
    the well-known LARS (or homotopy) procedure in large lasso problems , and that it
    can be applied to related methods such as the garotte and elastic net. It turns out
    that coordinate-wise descent does not work in the ``fused lasso,'' however, so we
    derive a generalized algorithm that yields the solution in much less time that a
    standard convex optimizer. Finally, we generalize the procedure to the
    two-dimensional fused lasso, and demonstrate its performance on some image
    smoothing problems.
  },
  langid        = {english}
}

% == BibLateX quality report for friedman2007:
% ? unused Library catalog ("Project Euclid")
@article{friedman2010,
  title         = {Regularization Paths for Generalized Linear Models via Coordinate Descent},
  author        = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
  volume        = {33},
  number        = {1},
  pages         = {1--22},
  doi           = {10.18637/jss.v033.i01},
  url           = {http://www.jstatsoft.org/v33/i01/},
  date          = {2010-01},
  journaltitle  = {Journal of Statistical Software}
}

@article{hare2004,
  title         = {
    Identifying {{Active Constraints}} via {{Partial Smoothness}} and {{
    Prox-Regularity}}
  },
  author        = {Hare, W L and Lewis, A S},
  volume        = {11},
  number        = {2},
  pages         = {16},
  date          = {2004},
  journaltitle  = {Journal of Convex Analysis},
  abstract      = {
    Active set algorithms, such as the projected gradient method in nonlinear
    optimization, are designed to ``identify'' the active constraints of the problem in
    a finite number of iterations. Using the notions of ``partial smoothness'' and
    ``prox-regularity'' we extend work of Burke, Mor\textasciiacute{}e and Wright on
    identifiable surfaces from the convex case to a general nonsmooth setting. We
    further show how this setting can be used in the study of sufficient conditions for
    local minimizers.
  },
  langid        = {english}
}

% == BibLateX quality report for hare2004:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Zotero")
@article{jiang2022,
  title         = {Adaptive {{Bayesian SLOPE}}: Model Selection with Incomplete Data},
  shorttitle    = {Adaptive {{Bayesian SLOPE}}},
  author        = {
    Jiang, Wei and Bogdan, Ma\l{}gorzata and Josse, Julie and Majewski, Szymon and
    Miasojedow, B\l{}a\.{z}ej and Ro\v{c}kov\'{a}, Veronika
  },
  publisher     = {{Taylor \& Francis}},
  volume        = {31},
  number        = {1},
  pages         = {113--137},
  doi           = {10.1080/10618600.2021.1963263},
  issn          = {1061-8600},
  url           = {https://doi.org/10.1080/10618600.2021.1963263},
  urldate       = {2022-06-03},
  date          = {2022-01-02},
  journaltitle  = {Journal of Computational and Graphical Statistics},
  abstract      = {
    We consider the problem of variable selection in high-dimensional settings with
    missing observations among the covariates. To address this relatively understudied
    problem, we propose a new synergistic procedure--adaptive Bayesian SLOPE with
    missing values--which effectively combines SLOPE (sorted l1 regularization) with
    the spike-and-slab LASSO (SSL) and is accompanied by an efficient stochastic
    approximation of expected maximization (SAEM) algorithm to handle missing data.
    Similarly as in SSL, the regression coefficients are regarded as arising from a
    hierarchical model consisting of two groups: the spike for the inactive and the
    slab for the active. However, instead of assigning independent spike and slab
    Laplace priors for each covariate, here we deploy a joint SLOPE ``spike-and-slab''
    prior which takes into account the ordering of coefficient magnitudes in order to
    control for false discoveries. We position our approach within a Bayesian framework
    which allows for simultaneous variable selection and parameter estimation while
    handling missing data. Through extensive simulations, we demonstrate satisfactory
    performance in terms of power, false discovery rate (FDR) and estimation bias under
    a wide range of scenarios including complete data and existence of missingness.
    Finally, we analyze a real dataset consisting of patients from Paris hospitals who
    underwent severe trauma, where we show competitive performance in predicting
    platelet levels. Our methodology has been implemented in C++ and wrapped into open
    source R programs for public use. Supplemental files for this article are available
    online.
  }
}

% == BibLateX quality report for jiang2022:
% Unexpected field 'publisher'
% ? unused Library catalog ("Taylor and Francis+NEJM")
@article{kos2020,
  title         = {On the Asymptotic Properties of {{SLOPE}}},
  author        = {Kos, Micha\l{} and Bogdan, Ma\l{}gorzata},
  volume        = {82},
  number        = {2},
  pages         = {499--532},
  doi           = {10.1007/s13171-020-00212-5},
  issn          = {0976-8378},
  url           = {https://doi.org/10.1007/s13171-020-00212-5},
  urldate       = {2022-06-03},
  date          = {2020-08-11},
  journaltitle  = {Sankhya A},
  shortjournal  = {Sankhya A},
  abstract      = {
    Sorted L-One Penalized Estimator (SLOPE) is a relatively new convex optimization
    procedure for selecting predictors in high dimensional regression analyses. SLOPE
    extends LASSO by replacing the L1 penalty norm with a Sorted L1 norm, based on the
    non-increasing sequence of tuning parameters. This allows SLOPE to adapt to unknown
    sparsity and achieve an asymptotic minimax convergency rate under a wide range of
    high dimensional generalized linear models. Additionally, in the case when the
    design matrix is orthogonal, SLOPE with the sequence of tuning parameters
    \ensuremath{\lambda}BH corresponding to the sequence of decaying thresholds for the
    Benjamini-Hochberg multiple testing correction provably controls the False
    Discovery Rate (FDR) in the multiple regression model. In this article we provide
    new asymptotic results on the properties of SLOPE when the elements of the design
    matrix are iid random variables from the Gaussian distribution. Specifically, we
    provide conditions under which the asymptotic FDR of SLOPE based on the sequence
    \ensuremath{\lambda}BH converges to zero and the power converges to 1. We
    illustrate our theoretical asymptotic results with an extensive simulation study.
    We also provide precise formulas describing FDR of SLOPE under different loss
    functions, which sets the stage for future investigation on the model selection
    properties of SLOPE and its extensions.
  },
  langid        = {english}
}

@inproceedings{larsson2020b,
  title         = {The Strong Screening Rule for {{SLOPE}}},
  author        = {Larsson, Johan and Bogdan, Ma\l{}gorzata and Wallin, Jonas},
  booktitle     = {Advances in Neural Information Processing Systems 33},
  location      = {{Virtual}},
  publisher     = {{Curran Associates, Inc.}},
  volume        = {33},
  pages         = {14592--14603},
  isbn          = {978-1-71382-954-6},
  url           = {
    https://papers.nips.cc/paper\%5Ffiles/paper/2020/hash/a7d8ae4569120b5bec12e7b6e9648b86-Abstract.html
  },
  editor        = {
    Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan,
    Maria-Florina and Lin, Hsuan-Tien
  },
  date          = {2020-12-06/2020-12-12},
  abstract      = {
    Extracting relevant features from data sets where the number of observations n is
    much smaller then the number of predictors p is a major challenge in modern
    statistics. Sorted L-One Penalized Estimation (SLOPE)--a generalization of the
    lasso---is a promising method within this setting. Current numerical procedures for
    SLOPE, however, lack the efficiency that respective tools for the lasso enjoy,
    particularly in the context of estimating a complete regularization path. A key
    component in the efficiency of the lasso is predictor screening rules: rules that
    allow  predictors to be discarded before estimating the model. This is the first
    paper to establish such a rule for SLOPE. We develop a screening rule for SLOPE by
    examining its subdifferential and show that this rule is a generalization of the
    strong rule for the lasso. Our rule is heuristic, which means that it may discard
    predictors erroneously. In our paper, however, we show that such situations are
    rare and easily safeguarded against by a simple check of the optimality conditions.
    Our numerical experiments show that the rule performs well in practice, leading to
    improvements by orders of magnitude for data in the \textbackslash (p
    \textbackslash gg n\textbackslash ) domain, as well as incurring no additional
    computational overhead when \$n {$>\$} p\$.
  },
  eventtitle    = {34th Conference on Neural Information Processing Systems ({{NeurIPS}} 2020)},
  langid        = {english}
}

% == BibLateX quality report for larsson2020b:
% ? Unsure about the formatting of the booktitle

% == BibLateX quality report for larsson2020b:
% ? Unsure about the formatting of the booktitle
@article{lewis2002a,
  title         = {Active {{Sets}}, {{Nonsmoothness}}, and {{Sensitivity}}},
  author        = {Lewis, A. S.},
  volume        = {13},
  number        = {3},
  pages         = {702--725},
  doi           = {10.1137/S1052623401387623},
  issn          = {1052-6234, 1095-7189},
  url           = {http://epubs.siam.org/doi/10.1137/S1052623401387623},
  urldate       = {2022-10-19},
  date          = {2002-01},
  journaltitle  = {SIAM Journal on Optimization},
  shortjournal  = {SIAM J. Optim.},
  abstract      = {
    Nonsmoothness pervades optimization, but the way it typically arises is highly
    structured. Nonsmooth behavior of an objective function is usually associated,
    locally, with an active manifold: on this manifold the function is smooth, whereas
    in normal directions it is ``veeshaped.'' Active set ideas in optimization depend
    heavily on this structure. Important examples of such functions include the
    pointwise maximum of some smooth functions and the maximum eigenvalue of a
    parametrized symmetric matrix. Among possible foundations for practical nonsmooth
    optimization, this broad class of ``partly smooth'' functions seems a promising
    candidate, enjoying a powerful calculus and sensitivity theory. In particular, we
    show under a natural regularity condition that critical points of partly smooth
    functions are stable: small perturbations to the function cause small movements of
    the critical point on the active manifold.
  },
  langid        = {english}
}

% == BibLateX quality report for lewis2002a:
% 'issn': not a valid ISSN
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("DOI.org (Crossref)")
@article{luo2019,
  title         = {
    Solving the {{OSCAR}} and {{SLOPE}} Models Using a Semismooth {{ Newton-based}}
    Augmented {{Lagrangian}} Method
  },
  author        = {Luo, Ziyan and Sun, Defeng and Toh, Kim-Chuan and Xiu, Naihua},
  volume        = {20},
  number        = {106},
  pages         = {1--25},
  issn          = {1533-7928},
  url           = {http://jmlr.org/papers/v20/18-172.html},
  urldate       = {2020-03-06},
  date          = {2019},
  journaltitle  = {Journal of Machine Learning Research},
  langid        = {english}
}

% == BibLateX quality report for luo2019:
% ? unused Library catalog ("www.jmlr.org")
@article{nesterov2012,
  title         = {Efficiency of Coordinate Descent Methods on Huge-Scale Optimization Problems},
  author        = {family=Nesterov, given=Yu., given-i={{Yu}}},
  publisher     = {{Society for Industrial and Applied Mathematics}},
  volume        = {22},
  number        = {2},
  pages         = {341--362},
  doi           = {10.1137/100802001},
  issn          = {1052-6234},
  url           = {https://epubs.siam.org/doi/abs/10.1137/100802001},
  urldate       = {2022-02-03},
  date          = {2012-01-01},
  journaltitle  = {SIAM Journal on Optimization},
  shortjournal  = {SIAM J. Optim.},
  abstract      = {
    In this paper we propose new methods for solving huge-scale optimization problems.
    For problems of this size, even the simplest full-dimensional vector operations are
    very expensive. Hence, we propose to apply an optimization technique based on
    random partial update of decision variables. For these methods, we prove the global
    estimates for the rate of convergence. Surprisingly, for certain classes of
    objective functions, our results are better than the standard worst-case bounds for
    deterministic algorithms. We present constrained and unconstrained versions of the
    method and its accelerated variant. Our numerical test confirms a high efficiency
    of this technique on problems of very big size.
  }
}

% == BibLateX quality report for nesterov2012:
% Unexpected field 'publisher'
% ? unused Library catalog ("epubs.siam.org (Atypon)")
@article{paige1982,
  title         = {{{LSQR}}: An Algorithm for Sparse Linear Equations and Sparse Least Squares},
  shorttitle    = {{{LSQR}}},
  author        = {Paige, Christopher C. and Saunders, Michael A.},
  volume        = {8},
  number        = {1},
  pages         = {43--71},
  doi           = {10.1145/355984.355989},
  issn          = {0098-3500},
  url           = {https://doi.org/10.1145/355984.355989},
  urldate       = {2022-10-11},
  date          = {1982-03-01},
  journaltitle  = {ACM Transactions on Mathematical Software},
  shortjournal  = {ACM Trans. Math. Softw.}
}

% == BibLateX quality report for paige1982:
% ? unused Library catalog ("March 1982")
@online{pedregosa2022,
  title         = {Preprocessing Data},
  author        = {
    Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and
    Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and
    Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and
    Duchesnay, E.
  },
  url           = {https://scikit-learn/stable/modules/preprocessing.html},
  urldate       = {2023-01-29},
  date          = {2022-09-16},
  langid        = {english},
  organization  = {{scikit-learn}}
}

@article{rhee2006,
  title         = {Genotypic Predictors of Human Immunodeficiency Virus Type 1 Drug Resistance},
  author        = {
    Rhee, Soo-Yon and Taylor, Jonathan and Wadhera, Gauhar and Ben-Hur, Asa and
    Brutlag, Douglas L. and Shafer, Robert W.
  },
  publisher     = {{Proceedings of the National Academy of Sciences}},
  volume        = {103},
  number        = {46},
  pages         = {17355--17360},
  doi           = {10.1073/pnas.0607274103},
  url           = {https://www.pnas.org/doi/abs/10.1073/pnas.0607274103},
  urldate       = {2022-10-05},
  date          = {2006-11-14},
  journaltitle  = {Proceedings of the National Academy of Sciences}
}

% == BibLateX quality report for rhee2006:
% Unexpected field 'publisher'
% ? unused Library catalog ("pnas.org (Atypon)")
@article{richtarik2014,
  title         = {
    Iteration Complexity of Randomized Block-Coordinate Descent Methods for Minimizing
    a Composite Function
  },
  author        = {Richt\'{a}rik, Peter and Tak\'{a}\v{c}, Martin},
  publisher     = {{Springer Nature}},
  volume        = {144},
  number        = {1/2},
  pages         = {1--38},
  doi           = {10.1007/s10107-012-0614-z},
  issn          = {00255610},
  url           = {
    https://ludwig.lub.lu.se/login?url=https://search.ebscohost.com/login.aspx?direct=true\&AuthType=ip
    ,uid\&db=bth\&AN=94971337\&site=eds-live\&scope=site
  },
  urldate       = {2022-10-19},
  date          = {2014-04},
  journaltitle  = {Mathematical Programming},
  shortjournal  = {Math. Program.},
  abstract      = {
    In this paper we develop a randomized block-coordinate descent method for
    minimizing the sum of a smooth and a simple nonsmooth block-separable convex
    function and prove that it obtains an \$\$ \textbackslash varepsilon \$\$-accurate
    solution with probability at least \$\$1-\textbackslash rho \$\$ in at most
    \$\$O((n/ \textbackslash varepsilon ) \textbackslash log (1/\textbackslash rho
    ))\$\$ iterations, where \$\$n\$\$ is the number of blocks. This extends recent
    results of Nesterov (SIAM J Optim 22(2): 341-362, 2012), which cover the smooth
    case, to composite minimization, while at the same time improving the complexity by
    the factor of 4 and removing \$\$\textbackslash varepsilon \$\$ from the
    logarithmic term. More importantly, in contrast with the aforementioned work in
    which the author achieves the results by applying the method to a regularized
    version of the objective function with an unknown scaling factor, we show that this
    is not necessary, thus achieving first true iteration complexity bounds. For
    strongly convex functions the method converges linearly. In the smooth case we also
    allow for arbitrary probability vectors and non-Euclidean norms. Finally, we
    demonstrate numerically that the algorithm is able to solve huge-scale
    \$\$\textbackslash ell \_1\$ \$-regularized least squares problems with a billion
    variables.
  }
}

% == BibLateX quality report for richtarik2014:
% Unexpected field 'publisher'
% ? unused Library catalog ("EBSCOhost")
@book{rockafellar1970,
  title         = {Convex Analysis},
  author        = {Rockafellar, R. Tyrrell},
  publisher     = {{Princeton University Press}},
  series        = {Princeton {{Landmarks}} in {{Mathematics}} and {{Physics}}},
  isbn          = {978-0-691-01586-6},
  url           = {https://www.jstor.org/stable/j.ctt14bs1ff},
  urldate       = {2022-08-31},
  date          = {1970},
  eprint        = {j.ctt14bs1ff},
  eprinttype    = {jstor},
  abstract      = {
    Available for the first time in paperback, R. Tyrrell Rockafellar's classic study
    presents readers with a coherent branch of nonlinear mathematical analysis that is
    especially suited to the study of optimization problems. Rockafellar's theory
    differs from classical analysis in that differentiability assumptions are replaced
    by convexity assumptions. The topics treated in this volume include: systems of
    inequalities, the minimum or maximum of a convex function over a convex set,
    Lagrange multipliers, minimax theorems and duality, as well as basic results about
    the structure of convex sets and the continuity and differentiability of convex
    functions and saddle- functions. This book has firmly established a new and vital
    area not only for pure mathematics but also for applications to economics and
    engineering. A sound knowledge of linear algebra and introductory real analysis
    should provide readers with sufficient background for this book. There is also a
    guide for the reader who may be using the book as an introduction, indicating which
    parts are essential and which may be skipped on a first reading.
  },
  langid        = {english},
  pagetotal     = {472}
}

@unpublished{schneider2020a,
  title         = {The Geometry of Uniqueness, Sparsity and Clustering in Penalized Estimation},
  author        = {Schneider, Ulrike and Tardivel, Patrick},
  publisher     = {{arXiv}},
  number        = {arXiv:2004.09106},
  pages         = {34},
  doi           = {10.48550/arXiv.2004.09106},
  url           = {http://arxiv.org/abs/2004.09106},
  urldate       = {2022-06-03},
  date          = {2020-08-18},
  eprint        = {2004.09106},
  eprinttype    = {arxiv},
  eprintclass   = {math, stat},
  abstract      = {
    We provide a necessary and sufficient condition for the uniqueness of penalized
    least-squares estimators whose penalty term is given by a norm with a polytope unit
    ball, covering a wide range of methods including SLOPE and LASSO, as well as the
    related method of basis pursuit. We consider a strong type of uniqueness that is
    relevant for statistical problems. The uniqueness condition is geometric and
    involves how the row span of the design matrix intersects the faces of the dual
    norm unit ball, which for SLOPE is given by the sign permutahedron. Further
    considerations based this condition also allow to derive results on sparsity and
    clustering features. In particular, we define the notion of a SLOPE model to
    describe both sparsity and clustering properties of this method and also provide a
    geometric characterization of accessible SLOPE models.
  }
}

% == BibLateX quality report for schneider2020a:
% Unexpected field 'number'
% Unexpected field 'pages'
% Unexpected field 'publisher'
@unpublished{shi2017,
  title         = {A Primer on Coordinate Descent Algorithms},
  author        = {Shi, Hao-Jun Michael and Tu, Shenyinying and Xu, Yangyang and Yin, Wotao},
  url           = {http://arxiv.org/abs/1610.00040},
  urldate       = {2022-02-03},
  date          = {2017-01-12},
  eprint        = {1610.00040},
  eprinttype    = {arxiv},
  eprintclass   = {math, stat},
  abstract      = {
    This monograph presents a class of algorithms called coordinate descent algorithms
    for mathematicians, statisticians, and engineers outside the field of optimization.
    This particular class of algorithms has recently gained popularity due to their
    effectiveness in solving large-scale optimization problems in machine learning,
    compressed sensing, image processing, and computational statistics. Coordinate
    descent algorithms solve optimization problems by successively minimizing along
    each coordinate or coordinate hyperplane, which is ideal for parallelized and
    distributed computing. Avoiding detailed technicalities and proofs, this monograph
    gives relevant theory and examples for practitioners to effectively apply
    coordinate descent to modern problems in data science and engineering.
  }
}

@article{tseng2001,
  title         = {
    Convergence of a Block Coordinate Descent Method for Nondifferentiable Minimization
  },
  author        = {Tseng, P.},
  volume        = {109},
  number        = {3},
  pages         = {475--494},
  doi           = {10.1023/A:1017501703105},
  issn          = {1573-2878},
  url           = {https://doi.org/10.1023/A:1017501703105},
  urldate       = {2022-09-16},
  date          = {2001-06-01},
  journaltitle  = {Journal of Optimization Theory and Applications},
  abstract      = {
    We study the convergence properties of a (block) coordinate descent method applied
    to minimize a nondifferentiable (nonconvex) function f(x1, . . . , xN) with certain
    separability and regularity properties. Assuming that f is continuous on a compact
    level set, the subsequence convergence of the iterates to a stationary point is
    shown when either f is pseudoconvex in every pair of coordinate blocks from among
    N-1 coordinate blocks or f has at most one minimum in each of N-2 coordinate
    blocks. If f is quasiconvex and hemivariate in every coordinate block, then the
    assumptions of continuity of f and compactness of the level set may be relaxed
    further. These results are applied to derive new (and old) convergence results for
    the proximal minimization algorithm, an algorithm of Arimoto and Blahut, and an
    algorithm of Han. They are applied also to a problem of blind source separation.
  },
  langid        = {english}
}

% == BibLateX quality report for tseng2001:
% ? unused Library catalog ("Springer Link")
@article{tseng2009,
  title         = {A Coordinate Gradient Descent Method for Nonsmooth Separable Minimization},
  author        = {Tseng, Paul and Yun, Sangwoon},
  publisher     = {{Springer Nature}},
  volume        = {117},
  number        = {1/2},
  pages         = {387--423},
  doi           = {10.1007/s10107-007-0170-0},
  issn          = {00255610},
  url           = {
    http://ludwig.lub.lu.se/login?url=https://search.ebscohost.com/login.aspx?direct=true\&AuthType=ip
    ,uid\&db=bth\&AN=32960989\&site=eds-live\&scope=site
  },
  urldate       = {2022-02-03},
  date          = {2009-03},
  journaltitle  = {Mathematical Programming},
  abstract      = {
    We consider the problem of minimizing the sum of a smooth function and a separable
    convex function. This problem includes as special cases bound-constrained
    optimization and smooth optimization with \mathscr{l}1-regularization. We propose a
    (block) coordinate gradient descent method for solving this class of nonsmooth
    separable problems. We establish global convergence and, under a local Lipschitzian
    error bound assumption, linear convergence for this method. The local Lipschitzian
    error bound holds under assumptions analogous to those for constrained smooth
    optimization, e.g., the convex function is polyhedral and the smooth function is
    (nonconvex) quadratic or is the composition of a strongly convex function with a
    linear mapping. We report numerical experience with solving the
    \mathscr{l}1-regularization of unconstrained optimization problems from Mor\'{e} et
    al. in ACM Trans. Math. Softw. 7, 17‚Äì41, 1981 and from the CUTEr set (Gould and
    Orban in ACM Trans. Math. Softw. 29, 373‚Äì394, 2003). Comparison with L-BFGS-B and
    MINOS, applied to a reformulation of the \mathscr{l}1-regularized problem as a
    bound-constrained optimization problem, is also reported.
  }
}

% == BibLateX quality report for tseng2009:
% Unexpected field 'publisher'
% ? unused Library catalog ("EBSCOhost")
@article{wright2015,
  title         = {Coordinate Descent Algorithms},
  author        = {Wright, Stephen},
  publisher     = {{Springer Nature}},
  volume        = {151},
  number        = {1},
  pages         = {3--34},
  doi           = {10.1007/s10107-015-0892-3},
  issn          = {00255610},
  url           = {
    http://ludwig.lub.lu.se/login?url=https://search.ebscohost.com/login.aspx?direct=true\&AuthType=ip
    ,uid\&db=bth\&AN=102704016\&site=eds-live\&scope=site
  },
  urldate       = {2022-02-03},
  date          = {2015-06},
  journaltitle  = {Mathematical Programming},
  abstract      = {
    Coordinate descent algorithms solve optimization problems by successively
    performing approximate minimization along coordinate directions or coordinate
    hyperplanes. They have been used in applications for many years, and their
    popularity continues to grow because of their usefulness in data analysis, machine
    learning, and other areas of current interest. This paper describes the
    fundamentals of the coordinate descent approach, together with variants and
    extensions and their convergence properties, mostly with reference to convex
    objectives. We pay particular attention to a certain problem structure that arises
    frequently in machine learning applications, showing that efficient implementations
    of accelerated coordinate descent algorithms are possible for problems of this
    type. We also present some parallel variants and discuss their convergence
    properties under several models of parallel execution.
  }
}

% == BibLateX quality report for wright2015:
% Unexpected field 'publisher'
% ? unused Library catalog ("EBSCOhost")
@book{zangwill1969,
  title         = {Nonlinear Programming: A Unified Approach},
  shorttitle    = {Nonlinear {{Programming}}},
  author        = {Zangwill, Willard I.},
  location      = {{New Orleans, USA}},
  publisher     = {{Prentice-Hall}},
  isbn          = {978-0-13-623579-8},
  date          = {1969},
  edition       = {1},
  eprint        = {TWhxLcApH9sC},
  eprinttype    = {googlebooks},
  langid        = {english},
  pagetotal     = {384}
}

@unpublished{zeng2015,
  title         = {The Ordered Weighted L1 Norm: Atomic Formulation, Projections, and Algorithms},
  shorttitle    = {The {{Ordered Weighted L1 Norm}}},
  author        = {Zeng, Xiangrong and Figueiredo, M\'{a}rio A. T.},
  url           = {http://arxiv.org/abs/1409.4271},
  urldate       = {2019-11-22},
  date          = {2015-04-10},
  eprint        = {1409.4271},
  eprinttype    = {arxiv},
  eprintclass   = {cs, math},
  abstract      = {
    The ordered weighted \$\textbackslash ell\_1\$ norm (OWL) was recently proposed,
    with two different motivations: its good statistical properties as a sparsity
    promoting regularizer; the fact that it generalizes the so-called \{\textbackslash
    it octagonal shrinkage and clustering algorithm for regression\} (OSCAR), which has
    the ability to cluster/group regression variables that are highly correlated. This
    paper contains several contributions to the study and application of OWL
    regularization: the derivation of the atomic formulation of the OWL norm; the
    derivation of the dual of the OWL norm, based on its atomic formulation; a new and
    simpler derivation of the proximity operator of the OWL norm; an efficient scheme
    to compute the Euclidean projection onto an OWL ball; the instantiation of the
    conditional gradient (CG, also known as Frank-Wolfe) algorithm for linear
    regression problems under OWL regularization; the instantiation of accelerated
    projected gradient algorithms for the same class of problems. Finally, a set of
    experiments give evidence that accelerated projected gradient algorithms are
    considerably faster than CG, for the class of problems considered.
  }
}

@article{zhang2010,
  title         = {Nearly Unbiased Variable Selection under Minimax Concave Penalty},
  author        = {Zhang, Cun-Hui},
  volume        = {38},
  number        = {2},
  pages         = {894--942},
  doi           = {10/bp22zz},
  issn          = {0090-5364, 2168-8966},
  url           = {https://projecteuclid.org/euclid.aos/1266586618},
  urldate       = {2018-03-14},
  date          = {2010-04},
  journaltitle  = {The Annals of Statistics},
  shortjournal  = {Ann. Statist.},
  abstract      = {
    We propose MC+, a fast, continuous, nearly unbiased and accurate method of
    penalized variable selection in high-dimensional linear regression. The LASSO is
    fast and continuous, but biased. The bias of the LASSO may prevent consistent
    variable selection. Subset selection is unbiased but computationally costly. The
    MC+ has two elements: a minimax concave penalty (MCP) and a penalized linear
    unbiased selection (PLUS) algorithm. The MCP provides the convexity of the
    penalized loss in sparse regions to the greatest extent given certain thresholds
    for variable selection and unbiasedness. The PLUS computes multiple exact local
    minimizers of a possibly nonconvex penalized loss function in a certain main branch
    of the graph of critical points of the penalized loss. Its output is a continuous
    piecewise linear path encompassing from the origin for infinite penalty to a least
    squares solution for zero penalty. We prove that at a universal penalty level, the
    MC+ has high probability of matching the signs of the unknowns, and thus correct
    selection, without assuming the strong irrepresentable condition required by the
    LASSO. This selection consistency applies to the case of p\gg{}n, and is proved to
    hold for exactly the MC+ solution among possibly many local minimizers. We prove
    that the MC+ attains certain minimax convergence rates in probability for the
    estimation of regression coefficients in \mathscr{l}r balls. We use the SURE method
    to derive degrees of freedom and Cp-type risk estimates for general penalized LSE,
    including the LASSO and MC+ estimators, and prove their unbiasedness. Based on the
    estimated degrees of freedom, we propose an estimator of the noise level for proper
    choice of the penalty level. For full rank designs and general sub-quadratic
    penalties, we provide necessary and sufficient conditions for the continuity of the
    penalized LSE. Simulation results overwhelmingly support our claim of superior
    variable selection properties and demonstrate the computational efficiency of the
    proposed method.
  },
  langid        = {english},
  mrnumber      = {MR2604701},
  zmnumber      = {1183.62120}
}

@inproceedings{larsson2022b,
  title         = {The {{Hessian}} Screening Rule},
  author        = {Larsson, Johan and Wallin, Jonas},
  booktitle     = {Advances in Neural Information Processing Systems 35},
  location      = {{New Orleans, USA}},
  publisher     = {{Curran Associates, Inc.}},
  volume        = {35},
  pages         = {15823--15835},
  isbn          = {978-1-71387-108-8},
  url           = {
    https://papers.nips.cc/paper\%5Ffiles/paper/2022/hash/65a925049647eab0aa06a9faf1cd470b-Abstract-Conference.html
  },
  editor        = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  date          = {2022-11-28/2022-12-09},
  abstract      = {
    Predictor screening rules, which discard predictors from the design matrix before
    fitting a model, have had considerable impact on the speed with which
    l1-regularized regression problems, such as the lasso, can be solved. Current
    state-of-the-art screening rules, however, have difficulties in dealing with
    highly-correlated predictors, often becoming too conservative. In this paper, we
    present a new screening rule to deal with this issue: the Hessian Screening Rule.
    The rule uses second-order information from the model to provide more accurate
    screening as well as higher-quality warm starts. The proposed rule outperforms all
    studied alternatives on data sets with high correlation for both l1-regularized
    least-squares (the lasso) and logistic regression. It also performs best overall on
    the real data sets that we examine.
  },
  eventtitle    = {
    36th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}} 2022)
  },
  langid        = {english}
}

@inproceedings{moreau2022a,
  title         = {Benchopt: Reproducible, Efficient and Collaborative Optimization Benchmarks},
  shorttitle    = {Benchopt},
  author        = {
    Moreau, Thomas and Massias, Mathurin and Gramfort, Alexandre and Ablin, Pierre and
    Bannier, Pierre-Antoine and Charlier, Benjamin and Dagr\'{e}ou, Mathieu and
    family=Tour, given=Tom Dupr\'{e}, prefix=la, useprefix=false and Durif, Ghislain
    and Dantas, Cassio F. and Klopfenstein, Quentin and Larsson, Johan and Lai, En and
    Lefort, Tanguy and Mal\'{e}zieux, Benoit and Moufad, Badr and Nguyen, Binh T. and
    Rakotomamonjy, Alain and Ramzi, Zaccharie and Salmon, Joseph and Vaiter, Samuel
  },
  booktitle     = {Advances in Neural Information Processing Systems 35},
  location      = {{New Orleans, USA}},
  publisher     = {{Curran Associates, Inc.}},
  volume        = {35},
  pages         = {25404--25421},
  isbn          = {978-1-71387-108-8},
  url           = {
    https://proceedings.neurips.cc/paper\%5Ffiles/paper/2022/hash/a30769d9b62c9b94b72e21e0ca73f338-Abstract-Conference.html
  },
  editor        = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  date          = {2022-11-28/2022-12-09},
  abstract      = {
    Numerical validation is at the core of machine learning research as it allows to
    assess the actual impact of new methods, and to confirm the agreement between
    theory and practice. Yet, the rapid development of the field poses several
    challenges: researchers are confronted with a profusion of methods to compare,
    limited transparency and consensus on best practices, as well as tedious
    re-implementation work. As a result, validation is often very partial, which can
    lead to wrong conclusions that slow down the progress of research. We propose
    Benchopt, a collaborative framework to automate, reproduce and publish optimization
    benchmarks in machine learning across programming languages and hardware
    architectures. Benchopt simplifies benchmarking for the community by providing an
    off-the-shelf tool for running, sharing and extending experiments. To demonstrate
    its broad usability, we showcase benchmarks on three standard learning tasks:
    \$\textbackslash ell\_2\$-regularized logistic regression, Lasso, and ResNet18
    training for image classification. These benchmarks highlight key practical
    findings that give a more nuanced view of the state-of-the-art for these problems,
    showing that for practical evaluation, the devil is in the details. We hope that
    Benchopt will foster collaborative work in the community hence improving the
    reproducibility of research findings.
  },
  eventtitle    = {36th Conference on Neural Information Processing Systems ({{NeurIPS}} 2022)}
}

% == BibLateX quality report for moreau2022a:
% ? Unsure about the formatting of the booktitle
